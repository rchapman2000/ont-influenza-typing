// Creates a parameters file and a summary file to 
// be added to later
process Setup {
    input:
        val trimming

        val minReadLen

        val maxReadLen
        // The provided medaka model
        val model
        // The batch size option supplied to medaka
        val minOverlapLen
        // The header to write to the summary file.
        val summaryHeader
        // The output directory to be used.
        val outDir
        
    output:
        // The parameters file created.
        file "analysis-parameters.txt"
        // The blank summary file to be added to.
        file "stats-summary.csv"

    publishDir "${outDir}", mode: 'copy'

    script:
    /*
    Creates a parameters file (in case the user needs to look back at how they ran the analysis)
    as well as a blank summary file, which will later be populated with data from each
    sample.

    The parameters file contains:
        1. Whether Porechop Trimming was enabled.
        2. The minimum read length cutoff.
        3. The maximum read length cutoff (if supplied).
        4. The minimum read overlap parameter used for Miniasm.
        4. The medaka model provided.

    The summary file will always contain:
        1. The sample
        2. Raw Reads
        3. Average Raw Read Length
        4. Trimmed Reads
        5. Average Trimmed Read Length
        6. Number of Draft Contigs
        7. Average Draft Contig Lengths
        8. Number of Corrected Contigs
        9. Average Corrected Contig Length
    */
    """
    #!/bin/bash

    touch analysis-parameters.txt

    echo "Porechop Adapter Trimming: ${trimming}" >> analysis-parameters.txt
    echo "Minimum Read Length Cutoff : ${minReadLen}" >> analysis-parameters.txt
    echo "Maxmimum Read Length Cutoff : ${maxReadLen}" >> analysis-parameters.txt
    echo "Minimum Read Overlap Length for Assembly : ${minOverlapLen} bp" >> analysis-parameters.txt
    echo "Medaka Model : ${model}" >> analysis-parameters.txt

    touch stats-summary.csv

    echo "${summaryHeader}" > stats-summary.csv
    """
}

process QC_Report {
    input:
        tuple val(base), file(reads)

        val outDir

    output:
        file("Raw-Reads-QC.zip")

    publishDir "${outDir}/${base}-Intermediate-Files/", mode: "copy"

    script:
    """
    #!/bin/bash

    mkdir Raw-Reads-QC

    if [[ -s ${reads} ]]; then
        NanoPlot --fastq ${reads} -o Raw-Reads-QC
    fi

    zip -r Raw-Reads-QC.zip Raw-Reads-QC
    """
}

process QC_Report_Filtered {
    input:
        tuple val(base), file(reads)

        val outDir

    output:
        file("Post-Filtering-QC.zip")

    publishDir "${outDir}/${base}-Intermediate-Files/", mode: "copy"

    script:
    """
    #!/bin/bash

    mkdir Post-Filtering-QC

    if [[ -s ${reads} ]]; then
        NanoPlot --fastq ${reads} -o Post-Filtering-QC
    fi

    zip -r Post-Filtering-QC.zip Post-Filtering-QC
    """
}

process Collect_Raw_Read_Stats {
    input:
        tuple val(base), file(reads)

    output:
        tuple val(base), file(reads)

        env summary

    // The publishDir directive is ignored here because we want the reads
    // to be an output of this process, but to save space, we do not want
    // to make a copy of them.
    script:
    """
    #!/bin/bash

    raw_reads=\$((\$(zcat -f ${reads} | wc -l)/4)) 

    avg_raw_read_len=\$(bioawk -c fastx '{ totalBases += length(\$seq); totalReads++} END{print totalBases/totalReads}' ${reads})

    summary="${base},\$raw_reads,\$avg_raw_read_len"
    """
}
  

// Detects and trims ONT adapter/barcode sequences from raw reads using Porechop
process Porechop_Trimming {
    input:
        // Tuple contains the sample base name and
        // the reads to be trimmed
        tuple val(base), file(reads)
        // The name of the output directory to write
        // output files to.
        val outDir

        // A string containing previously generated
        // statistics to be appended to.
        val existingSummary

    output:
        // Tuple contains the sample base name and 
        // the trimmed reads.
        tuple val(base), file("${base}-trimmed.fastq.gz")
        // The trimming report generated by porechop
        file "${base}-porechop-report.txt.gz"
        // A string containing summary metrics generated 
        // during this process.
        env summary

    publishDir "${outDir}/${base}-Intermediate-Files/", mode: 'copy', pattern: "*.fastq.gz"
    publishDir "${outDir}/${base}-Intermediate-Files/", mode: 'copy', pattern: "*.txt.gz"

    script:
    /*
    Uses Porechop to detect and trim ONT adapter/barcodes sequences found
    within the provided reads. 

    As well, the number of raw reads, average raw read length, number of 
    trimmed reads, and average trimmed read length are calculated
    and added to the summary data.
    */
    """
    #!/bin/bash

    porechop -i ${reads} -o ${base}-trimmed.fastq --verbosity 1 > ${base}-porechop-report.txt

    trimmed_reads=\$((\$(zcat -f ${base}-trimmed.fastq | wc -l)/4))

    avg_trimmed_read_len=\$(bioawk -c fastx '{ totalBases += length(\$seq); totalReads++ } END{ print totalBases/totalReads }' ${base}-trimmed.fastq)

    gzip ${base}-trimmed.fastq
    gzip ${base}-porechop-report.txt

    summary="${existingSummary},\$trimmed_reads,\$avg_trimmed_read_len"
    """
}


process Length_Filtering {
    input:
        tuple val(base), file(reads)

        val minLenParam

        val maxLenParam

        val outDir
        // A string containing previously generated
        // statistics to be appended to.
        val existingSummary

    output:
        tuple val(base), file("${base}-length-filtered.fastq.gz")

        env summary

    publishDir "${outDir}/${base}-Intermediate-Files/Processed-Reads", mode: 'copy', pattern: "*.fastq.gz"

    script:
    """
    #!/bin/bash

    zcat -f ${reads} | chopper ${minLenParam} ${maxLenParam} > ${base}-length-filtered.fastq

    num_filt_reads=\$((\$(cat ${base}-length-filtered.fastq | wc -l)/4)) 

    avg_filt_read_len=\$(bioawk -c fastx '{ totalBases += length(\$seq); totalReads++} END{print totalBases/totalReads}' ${base}-length-filtered.fastq)

    gzip ${base}-length-filtered.fastq

    summary="${existingSummary},\$num_filt_reads,\$avg_filt_read_len"
    """

    //NanoFilt ${minLenParam} ${maxLenParam} ${reads} > ${base}-length-filtered.fastq
}


// Perfroms a De Novo assembly of the ONT reads using
// Miniasm + Racon
process Miniasm_Assembly {
    input: 
        tuple val(base), file(reads)

        val minReadLen

        val minOverlapLen

        val outDir

        val existingSummary

    output:

        tuple val(base), file(reads), file("${base}-draft-contigs.fasta")

        env summary

    publishDir "${outDir}/${base}-Intermediate-Files/", mode: "copy", pattern: "${base}-draft-contigs.fasta"


    script:
    """
    #!/bin/bash

    minimap2 -x ava-ont ${reads} ${reads} > ${base}-reads-overlap.paf

    miniasm -m ${minOverlapLen} -s ${minReadLen} -f ${reads} ${base}-reads-overlap.paf > ${base}-miniasm.gfa

    awk '\$1 ~/S/ {print \">\"\$2\"\\n\"\$3}' ${base}-miniasm.gfa > ${base}-unpolished-draft-contigs.fasta

    minimap2 -x map-ont ${base}-unpolished-draft-contigs.fasta ${reads} > ${base}-reads-contigs-overlap.paf

    if [[ -s "${base}-reads-contigs-overlap.paf" ]]; then
        racon ${reads} ${base}-reads-contigs-overlap.paf ${base}-unpolished-draft-contigs.fasta ${reads} > ${base}-draft-contigs.fasta
    else
        mv ${base}-unpolished-draft-contigs.fasta ${base}-draft-contigs.fasta
    fi

    num_contigs=\$(grep ">" ${base}-draft-contigs.fasta | wc -l)

    avg_contig_len=\$(bioawk -c fastx '{ totalBases += length(\$seq); totalReads++} END{print totalBases/totalReads}' ${base}-draft-contigs.fasta)

    summary="${existingSummary},\$num_contigs,\$avg_contig_len"
    """
}

// Uses Medaka to Polish the contigs.
process Medaka_Correct {
    input:
        // Tuple contains the sample base name,
        // the reads used to generate the draft assembly,
        // and the draft assembly.
        tuple val(base), file(reads), file(draft)
        // The name of the output directory to copy
        // generated output files.
        val outDir
        // The number of threads available to
        // compute using.
        val threads
        // The medaka model provided.
        val model
        // A string containing previously generated
        // statistics to be appended to.
        val existingSummary

    output:
        // Tuple contains the sample base name and the
        // corrected assembly in fasta format.
        tuple val(base), file("${base}-corrected-assembly.fasta")
        // A string containing statistics including
        // those generated during this step.
        env summary

    publishDir "${outDir}", mode: "copy"

    script:
    /*
    Uses the medaka_consensus pipeline to correct the draft contigs using the raw reads.

    If the medaka pipeline worked correctly, a file named 'consensus.fasta' will be
    present in the medaka output directory. If this file exists, then it is moved into
    a file named "SAMPLE-corrected-assembly.fasta". If not, an empty file is created (to
    prevent file not found errors).

    Finally, statistics on the number of corrected contigs and average corrected contig length
    are appended to the string containing existing statistics.
    */
    """
    #!/bin/bash

    medaka_consensus -i ${reads} -d ${draft} -o ${base}-medaka -t ${threads} -m ${model}

    if [[ -f "${base}-medaka/consensus.fasta" ]]; then
        mv ${base}-medaka/consensus.fasta ./${base}-corrected-assembly.fasta
    else
        touch ./${base}-corrected-assembly.fasta
    fi

    num_corrected_contigs=\$(grep ">" ${base}-corrected-assembly.fasta | wc -l)

    avg_corrected_contig_len=\$(bioawk -c fastx '{ totalBases += length(\$seq); totalReads++ } END{ print totalBases/totalReads }' ${base}-corrected-assembly.fasta)

    summary="${existingSummary},\$num_corrected_contigs,\$avg_corrected_contig_len"
    """
}

process Abricate_Typing {

    input:
        tuple val(base), file(contigs)

        val baseDir

        val outDir

        val existingSummary

    output:
        file "${base}-abricate-report.txt"

        env summary

    publishDir "${outDir}/${base}-Intermediate-Files", mode: "copy", pattern: "${base}-abricate-report.txt"


    script:
    """
    #!/bin/bash

    abricate --datadir ${baseDir}/data/ -db insaFluDB ${contigs} --quiet 1> ${base}-abricate-report.txt

    typing=\$(python3 ${baseDir}/scripts/parse_abricate_results.py -i ${base}-abricate-report.txt)

    summary="${existingSummary},\$typing"
    """


}

// Writes a line to the summary file for the sample.
process Write_Summary {
    input:
        // Tuple contains the sample basename and forward/reverse reads (the basename
        // is the only value important to this function).
        val summary
        // The output directory.
        val outDir

    script:
    /*
    The summary string containing the statistics collected as the pipeline
    was run are appended to the summary file.
    */
    """
    #!/bin/bash

    echo "${summary}" >> ${outDir}/stats-summary.csv

    """  

}