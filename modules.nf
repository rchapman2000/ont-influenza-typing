// Creates a parameters file and a summary file to 
// be added to later
process Setup {
    input:
        val inputType
        
        val trimming

        val minReadLen

        val maxReadLen
        // The provided medaka model
        val model
        // The batch size option supplied to medaka
        val minReadsToAssemble
        // The header to write to the summary file.
        val summaryHeader
        // The output directory to be used.
        val outDir
        
    output:
        // The parameters file created.
        file "analysis-parameters.txt"
        // The blank summary file to be added to.
        file "stats-summary.csv"

    publishDir "${outDir}", mode: 'copy'

    script:
    /*
    Creates a parameters file (in case the user needs to look back at how they ran the analysis)
    as well as a blank summary file, which will later be populated with data from each
    sample.

    The parameters file contains:
        1. The input file type.
        2. Whether Porechop Trimming was enabled.
        3. The minimum read length cutoff.
        4. The maximum read length cutoff (if supplied).
        5. The minimum read overlap parameter used for Miniasm.
        6. The medaka model provided.
    */
    """
    #!/bin/bash

    touch analysis-parameters.txt

    echo "Input Type: ${inputType}" >> analysis-parameters.txt
    echo "Porechop Adapter Trimming: ${trimming}" >> analysis-parameters.txt
    echo "Minimum Read Length Cutoff : ${minReadLen}" >> analysis-parameters.txt
    echo "Maxmimum Read Length Cutoff : ${maxReadLen}" >> analysis-parameters.txt
    echo "Selection Criteria for Assembly References : ${minReadsToAssemble}" >> analysis-parameters.txt
    echo "Medaka Model : ${model}" >> analysis-parameters.txt

    touch stats-summary.csv

    echo "${summaryHeader}" > stats-summary.csv
    """
}

process QC_Report {
    input:
        tuple val(base), file(reads)

        val outDir

    output:
        file("Raw-Reads-QC.zip")

    publishDir "${outDir}/${base}-Intermediate-Files/", mode: "copy"

    script:
    """
    #!/bin/bash

    mkdir Raw-Reads-QC

    if [[ -s ${reads} ]]; then
        NanoPlot --fastq ${reads} -o Raw-Reads-QC
    fi

    zip -r Raw-Reads-QC.zip Raw-Reads-QC
    """
}

process QC_Report_Filtered {
    input:
        tuple val(base), file(reads)

        val outDir

    output:
        file("Post-Filtering-QC.zip")

    publishDir "${outDir}/${base}-Intermediate-Files/", mode: "copy"

    script:
    """
    #!/bin/bash

    mkdir Post-Filtering-QC

    if [[ -s ${reads} ]]; then
        NanoPlot --fastq ${reads} -o Post-Filtering-QC
    fi

    zip -r Post-Filtering-QC.zip Post-Filtering-QC
    """
}

process Collect_Raw_Read_Stats {
    input:
        tuple val(base), file(reads)

    output:
        tuple val(base), file(reads)

        env summary

    // The publishDir directive is ignored here because we want the reads
    // to be an output of this process, but to save space, we do not want
    // to make a copy of them.
    script:
    """
    #!/bin/bash

    raw_reads=\$((\$(zcat -f ${reads} | wc -l)/4)) 

    avg_raw_read_len=\$(bioawk -c fastx '{ totalBases += length(\$seq); totalReads++} END{print totalBases/totalReads}' ${reads})

    summary="${base},\$raw_reads,\$avg_raw_read_len"
    """
}

process Collect_FASTA_Stats {
    input:
        tuple val(base), file(fasta)

        val baseDir
    
    output:
        tuple val(base), file(fasta)

        env summary

    // The publishDir directive is ignored here because we want the reads
    // to be an output of this process, but to save space, we do not want
    // to make a copy of them.
    script:
    """
    #!/bin/bash

    sequences=\$(grep ">" ${fasta} | wc -l) 

    coverage=\$(python3 ${baseDir}/scripts/calculate_genome_coverage.py -i ${fasta})

    summary="${base},\$sequences,\$coverage"
    """
}
  

// Detects and trims ONT adapter/barcode sequences from raw reads using Porechop
process Porechop_Trimming {
    input:
        // Tuple contains the sample base name and
        // the reads to be trimmed
        tuple val(base), file(reads)
        // The name of the output directory to write
        // output files to.
        val outDir

        // A string containing previously generated
        // statistics to be appended to.
        val existingSummary

    output:
        // Tuple contains the sample base name and 
        // the trimmed reads.
        tuple val(base), file("${base}-trimmed.fastq.gz")
        // The trimming report generated by porechop
        file "${base}-porechop-report.txt.gz"
        // A string containing summary metrics generated 
        // during this process.
        env summary

    publishDir "${outDir}/${base}-Intermediate-Files/", mode: 'copy', pattern: "*.fastq.gz"
    publishDir "${outDir}/${base}-Intermediate-Files/", mode: 'copy', pattern: "*.txt.gz"

    script:
    /*
    Uses Porechop to detect and trim ONT adapter/barcodes sequences found
    within the provided reads. 

    As well, the number of raw reads, average raw read length, number of 
    trimmed reads, and average trimmed read length are calculated
    and added to the summary data.
    */
    """
    #!/bin/bash

    porechop -i ${reads} -o ${base}-trimmed.fastq --verbosity 1 > ${base}-porechop-report.txt

    trimmed_reads=\$((\$(zcat -f ${base}-trimmed.fastq | wc -l)/4))

    avg_trimmed_read_len=\$(bioawk -c fastx '{ totalBases += length(\$seq); totalReads++ } END{ print totalBases/totalReads }' ${base}-trimmed.fastq)

    gzip ${base}-trimmed.fastq
    gzip ${base}-porechop-report.txt

    summary="${existingSummary},\$trimmed_reads,\$avg_trimmed_read_len"
    """
}


process Length_Filtering {
    input:
        tuple val(base), file(reads)

        val minLenParam

        val maxLenParam

        val outDir
        // A string containing previously generated
        // statistics to be appended to.
        val existingSummary

    output:
        tuple val(base), file("${base}-length-filtered.fastq.gz")

        env summary

    publishDir "${outDir}/${base}-Intermediate-Files/Processed-Reads", mode: 'copy', pattern: "*.fastq.gz"

    script:
    """
    #!/bin/bash

    zcat -f ${reads} | chopper ${minLenParam} ${maxLenParam} > ${base}-length-filtered.fastq

    num_filt_reads=\$((\$(cat ${base}-length-filtered.fastq | wc -l)/4)) 

    avg_filt_read_len=\$(bioawk -c fastx '{ totalBases += length(\$seq); totalReads++} END{print totalBases/totalReads}' ${base}-length-filtered.fastq)

    gzip ${base}-length-filtered.fastq

    summary="${existingSummary},\$num_filt_reads,\$avg_filt_read_len"
    """

    //NanoFilt ${minLenParam} ${maxLenParam} ${reads} > ${base}-length-filtered.fastq
}

process Select_References_For_Assembly {
    input:
        tuple val(base), file(reads)

        val minReadParam
        
        val baseDir

        val outDir
        
        val existingSummary

    output:
        tuple val(base), file(reads), file("${base}-references.fasta")

        file "${base}-reference-alignment-stats.tab"

        env summary

    publishDir "${outDir}/${base}-Intermediate-Files/", mode: "copy", pattern: "${base}-references-for-assembly.fasta"
    publishDir "${outDir}/${base}-Intermediate-Files/", mode: "copy", pattern: "${base}-reference-alignment-stats.tab"

    script:
    """
    #!/bin/bash

    minimap2 -ax map-ont --secondary=no ${baseDir}/data/insaFlu.fasta ${reads} > ${base}-classification.sam

    samtools view -b ${base}-classification.sam | samtools sort > ${base}-classification.bam

    samtools idxstats ${base}-classification.bam | cut -f 1,3 > ${base}-classification-stats.tab

    python3 ${baseDir}/scripts/select_flu_segment_references.py --alignmentStats ${base}-classification-stats.tab --referenceDB ${baseDir}/data/insaFlu.fasta --outPref ${base} ${minReadParam}

    if [[ -s ${base}-references.fasta ]]; then

        refs_used=\$(bioawk -c fastx '{printf \$name " "}' ${base}-references.fasta)

    else

        refs_used=""
        cat ${baseDir}/data/insaFlu.fasta > ${base}-references.fasta

    fi

    

    summary="${existingSummary},\$refs_used"
    """
}

process Minimap2_Alignment {
    input:
        tuple val(base), file(reads), file(ref)

        val outDir
        
        val existingSummary

    output:
        tuple val(base), file(ref), file("${base}-align.bam")

        env summary

    publishDir "${outDir}/${base}-Intermediate-Files/", mode: "copy", pattern: "${base}-align.bam"

    script:
    """
    #!/bin/bash

    minimap2 -ax map-ont --secondary=no ${ref} ${reads} > align.sam

    samtools view -b align.sam | samtools sort > ${base}-align.bam

    mapped_reads=\$(samtools view -F 0x04 -c ${base}-align.bam)

    average_read_depth=\$(samtools depth -a -J -q 0 -Q 0 ${base}-align.bam | awk -F'\t' 'BEGIN{totalCov=0} {totalCov+=\$3} END{print totalCov/NR}')

    summary="${existingSummary},\$mapped_reads,\$average_read_depth"

    """
    
}

process Medaka_Alignment_Polish {
    input:
        tuple val(base), file(ref), file(bam)

        val model

        val outDir

        val existingSummary

    output:
        tuple val(base), file(ref), file(bam), file("${base}-align.hdf")

        env summary

    publishDir "${outDir}/${base}-Intermediate-Files", mode: "copy", pattern: "*.hdf"

    script:
    """
    #!/bin/bash

    samtools index ${bam}

    medaka consensus ${bam} --model ${model} ${base}-align.hdf --debug

    summary="${existingSummary}"
    """

}

process Call_Variants {
    input:
        tuple val(base), file(ref), file(bam), file(hdf)

        val minCov

        val outDir

        val existingSummary

    output:
        tuple val(base), file(ref), file(bam), file("${base}-snps-filtered.vcf"), file("${base}-indels-filtered.vcf")

        env summary
    
    publishDir "${outDir}/${base}-Intermediate-Files/", mode: "copy", pattern: "*.vcf"

    script:
    """
    #!/bin/bash

    samtools index ${bam}

    medaka variant ${ref} ${hdf} ${base}-medaka.vcf

    bgzip ${base}-medaka.vcf
    tabix ${base}-medaka.vcf.gz

    longshot -P 0 -A -a 0 --min_mapq 0 --no_haps -v ${base}-medaka.vcf.gz --bam ${bam} --ref ${ref} --out ${base}-longshot.vcf

    vcftools --keep-only-indels --vcf ${base}-longshot.vcf --recode --recode-INFO-all --stdout > ${base}-indels.vcf
    bgzip ${base}-indels.vcf
    tabix ${base}-indels.vcf.gz

    vcftools --remove-indels --vcf ${base}-longshot.vcf --recode --recode-INFO-all --stdout > ${base}-snps.vcf
    bgzip ${base}-snps.vcf
    tabix ${base}-snps.vcf.gz

    bcftools view -i "((INFO/AC[0] + INFO/AC[1]) >= ${minCov}) && ((INFO/AC[1] / INFO/DP) > (INFO/AC[0] / INFO/DP))" ${base}-indels.vcf.gz > ${base}-indels-filtered.vcf
    num_indels=\$(grep -v "^#" ${base}-indels-filtered.vcf | wc -l)

    bcftools view -i "((INFO/AC[0] + INFO/AC[1]) >= ${minCov}) && ((INFO/AC[1] / INFO/DP) > (INFO/AC[0] / INFO/DP))" ${base}-snps.vcf.gz > ${base}-snps-filtered.vcf
    num_snps=\$(grep -v "^#" ${base}-snps-filtered.vcf | wc -l)

    summary="${existingSummary},\$num_snps,\$num_indels"
    """

}

process Generate_Consensus {
    input:
        // Tuple contains the file basename, the alignment bam, the snp vcf file
        // and the indel vcf file
        tuple val(base), file(ref), file(bam), file(snps), file(indels)
        // The name of the base directory
        val baseDir
        // The name of the base directory
        val outDir
        // The minimum coverage threshold
        val minCov
        // The existing summary string.
        val existingSummary

    output:
        // Tuple contains the consensus fasta and the sites that were masked in 
        // a bed file.
        tuple val(base), file("${base}-consensus.fasta")
        // The bed file containing the masked sites.
        file "${base}-mask-sites.bed"
        // The summary string with the number of masked positions and coverage
        // added.
        env summary

    publishDir "${outDir}", mode: 'copy', pattern: "${base}-consensus.fasta"
    publishDir "${outDir}/${base}-Intermediate-Files", mode: 'copy', pattern: "${base}-mask-sites.bed"

    script:
    /*
    The script first computes sites to mask by identifying sites that have less than
    the minimum coverage provided. However, there are formatting issues with the pileup
    format that make this difficult. Samtools mpileup's output has the depth in 0-based
    format, which makes it impossible to distinguish between a site with 0 and 1 coverage.

    Thus, the pipeline instead makes use of bedtools subtract. It first creates a pileup for only sites with
    coverage, uses an in-house script to filter sites with less than the minimum coverage
    and converts these into a bed file.

    Next, the pipeline creates a pileup containing every site, and converts this into a bed file using
    the in-house script. 

    Finally, the sites we want to keep (those above the minimum coverage threshold) are substracted
    from the bed file with every site, to give us the low-coverage sites.

    Additionally, there is an interesting case when the sites that fall within a deletion are marked as masked.
    Because masking is applied first, this will cause an error when applying the variants (as the deletion site will contain 
    an N character and will not match the VCF reference). Thus, the pipeline uses bcftools to create a bed file for all indel sites,
    and then subtracts these from the low coverage sites. Now, this results in the sites to mask.

    The bedtools maskfasta command is then used to mask the reference at these positions.

    Then the variants are applied to the mask fasta. The reason this is done after masking is 
    because the pileup (and therefore masking) positions do not account for indels, which would
    shift the genomic coordinates (we would end up masking things we did not want to).

    Finally, the fasta is wrapped to make it visually appealing. 

    Old Code:
    samtools mpileup --no-BAQ -d 100000 -x -A -a -Q 0 -f ${ref} ${bam} > all-sites.pileup
    python3 ${baseDir}/scripts/pileup_to_bed.py -i all-sites.pileup -o all-sites.bed 
    */
    """
    #!/bin/bash

    samtools mpileup --no-BAQ -d 100000 -x -A -Q 0 -f ${ref} ${bam} > ${base}.pileup
    python3 ${baseDir}/scripts/pileup_to_bed.py -i ${base}.pileup -o passed-sites.bed --minCov ${minCov}

    bioawk -c fastx '{print \$name"\t0\t"length(\$seq)}' ${ref} > all-sites.bed

    if [[ -s all-sites.bed ]]; then

        bedtools subtract -a all-sites.bed -b passed-sites.bed > ${base}-low-cov-sites.bed

        bcftools query -f'%CHROM\t%POS0\t%END\n' ${indels} > indel-sites.bed

        bedtools subtract -a ${base}-low-cov-sites.bed -b indel-sites.bed > ${base}-mask-sites.bed

    else
        bioawk -c fastx '{print \$name"\t0\t"length(\$seq)}' ${ref} > ${base}-mask-sites.bed
    fi

    num_mask=\$(bioawk -c bed 'BEGIN{SITES=0} {SITES+=\$end-\$start } END{print SITES}' ${base}-mask-sites.bed)

    bedtools maskfasta -fi ${ref} -bed ${base}-mask-sites.bed -fo masked.fasta

    bgzip ${snps}
    tabix ${snps}.gz

    bgzip ${indels}
    tabix ${indels}.gz

    bcftools consensus -f masked.fasta ${snps}.gz > with-snps.fasta

    bcftools consensus -f with-snps.fasta ${indels}.gz > with-indels-snps.fasta

    bioawk -c fastx '{ gsub(/\\n/,"",seq); print ">${base}-"\$name; print \$seq }' with-indels-snps.fasta > ${base}-consensus.fasta

    seq_len=\$(bioawk -c fastx 'BEGIN{bases=0} { bases+=length(\$seq) } END{print bases}' < ${base}-consensus.fasta)

    coverage=\$(python3 ${baseDir}/scripts/calculate_genome_coverage.py -i ${base}-consensus.fasta)

    summary="${existingSummary},\$num_mask,\$coverage"
    """
}

process Abricate_Typing {

    input:
        tuple val(base), file(contigs)

        val baseDir

        val outDir

        val existingSummary

    output:
        file "${base}-abricate-report.txt"

        env summary

    publishDir "${outDir}/${base}-Intermediate-Files", mode: "copy", pattern: "${base}-abricate-report.txt"


    script:
    """
    #!/bin/bash

    abricate --datadir ${baseDir}/data/ -db insaFluAbricateDB ${contigs} -minid 70 -mincov 60 --quiet 1> ${base}-abricate-report.txt

    typing=\$(python3 ${baseDir}/scripts/parse_abricate_results.py -i ${base}-abricate-report.txt)

    summary="${existingSummary},\$typing"
    """


}

// Writes a line to the summary file for the sample.
process Write_Summary {
    input:
        // Tuple contains the sample basename and forward/reverse reads (the basename
        // is the only value important to this function).
        val summary
        // The output directory.
        val outDir

    script:
    /*
    The summary string containing the statistics collected as the pipeline
    was run are appended to the summary file.
    */
    """
    #!/bin/bash

    echo "${summary}" >> ${outDir}/stats-summary.csv

    """  

}